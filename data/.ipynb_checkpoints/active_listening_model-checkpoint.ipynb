{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "import nltk\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from flashtext.keyword import KeywordProcessor\n",
    "import csv\n",
    "import keras\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Input, Embedding, Concatenate, concatenate\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.optimizers import SGD\n",
    "import en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "train_df = pd.read_csv('train_corpus.csv', encoding='latin1')\n",
    "labels_df = pd.read_csv('train_labels.csv', encoding='latin1')\n",
    "\n",
    "TRAIN_DATA = []\n",
    "\n",
    "labels = ['Used_appropriate_opening_and_prepared_for_the_call',\n",
    "          'Actively_listened_and_acknowledged_concerns',\n",
    "          'Expressed_sincere_and_appropriate_Empathy',\n",
    "          'Enthusiastic_and_Helpful_Tone',\n",
    "          'Confidence_and_demonstrated_ownership',\n",
    "          'Used_appropriate_closing',\n",
    "          'Integrity_and_Professionalism',\n",
    "          'Clear_and_easily_understood',\n",
    "          'Used_appropriate_word_choices_or_phrasing',\n",
    "          'Natural_use_of_customers_name_and_avoided_excessive_Sir_or_Maam',\n",
    "          'Maintained_control_of_the_call',\n",
    "          'Guided_the_call_towards_a_logical_resolution',\n",
    "          'Utilized_tools_and_resources_efficiently',\n",
    "          'Remained_focused_and_avoided_unexplained_dead_air',\n",
    "          'Clear_and_concise_notations',\n",
    "          'Reviewed_notes_or_history_and_probed_as_necessary',\n",
    "          'Processed_the customers_request_and_with_accuracy',\n",
    "          'Provided_correct_information_and_addressed_all_concerns',\n",
    "          'Followed_all_relevant_policy_and_procedures_including_customer_verification_and_product_up_sells']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(data):\n",
    "    data = word_tokenize(data)\n",
    "    data = [word for word in data if word.isalpha()]\n",
    "    data = [w for w in data if not w in stop_words]\n",
    "    data = [lemmatizer.lemmatize(w, pos='v') for w in data]\n",
    "    if len(data) == 0:\n",
    "        data = ['']\n",
    "    return data\n",
    "\n",
    "\n",
    "def imp_act(sent):\n",
    "    for word in sent:\n",
    "        if word.pos_ == 'VB':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "def qwh_act(sent):\n",
    "    for word in sent:\n",
    "        if word.pos_ == 'WDT' or word.pos_ == 'WP' or word.pos_ == 'WP$' or word.pos_ == 'WRB':\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def qyn_act(sent):\n",
    "    for word in sent:\n",
    "        if word.pos_ == 'MD':\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "\n",
    "def get_kp(file):\n",
    "    kp = KeywordProcessor()\n",
    "    with open('../data/intents/' + file + '.csv', mode='r') as infile:\n",
    "        reader = csv.reader(infile)\n",
    "        for read in reader:\n",
    "            kp.add_keyword(read[0], read[1])\n",
    "    return kp\n",
    "\n",
    "\n",
    "def accept(sent):\n",
    "    kp = get_kp('AcceptDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def acknowledge(sent):\n",
    "    kp = get_kp('AcknowledgeDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def agree(sent):\n",
    "    kp = get_kp('AgreeDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def badnews(sent):\n",
    "    kp = get_kp('BadNewsDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def clear(sent):\n",
    "    kp = get_kp('ClearDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def compliment(sent):\n",
    "    kp = get_kp('ComplimentDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def goodnews(sent):\n",
    "    kp = get_kp('GoodNewsDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def guess(sent):\n",
    "    kp = get_kp('GuessDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def hesitate(sent):\n",
    "    kp = get_kp('HesitateDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def interrupt(sent):\n",
    "    kp = get_kp('InterruptDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def negate(sent):\n",
    "    kp = get_kp('NegateDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def offer(sent):\n",
    "    kp = get_kp('OfferDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def pardon(sent):\n",
    "    kp = get_kp('PardonDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def promise(sent):\n",
    "    kp = get_kp('PromiseDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def refuse(sent):\n",
    "    kp = get_kp('RefuseDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def sympathy(sent):\n",
    "    kp = get_kp('SympathyDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def thank(sent):\n",
    "    kp = get_kp('ThankDialogueIntentImpl')\n",
    "    if len(kp.extract_keywords(sent)) > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "convs = []\n",
    "conv_features = []\n",
    "\n",
    "train_dials = []\n",
    "dials = []\n",
    "dial_features = []\n",
    "\n",
    "train_words = []\n",
    "word_features = []\n",
    "total_word_features = []\n",
    "corpus = []\n",
    "\n",
    "max_words = []\n",
    "max_dials = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in labels_df.values:\n",
    "    conv_features.append([row[16], row[17], row[20], row[22], row[27]])\n",
    "    rows = train_df.where(train_df['ConversationIDDataImpl'] == row[0])\n",
    "    rows = rows.dropna()\n",
    "    max_dials.append(len(rows))\n",
    "    call = ''\n",
    "    call_dials = []\n",
    "    call_dial_feats = []\n",
    "    call_words = []\n",
    "    call_word_feats = []\n",
    "    for val in rows.values:\n",
    "        dial = str(val[4]).strip().lower()\n",
    "        call += dial + ' '\n",
    "        call_dials.append(dial)\n",
    "        doc = nlp(str(val[4]).strip().lower())\n",
    "        call_dial_feats.append([val[1], val[16], imp_act(doc), qwh_act(doc), qyn_act(doc), accept(dial),\n",
    "                                acknowledge(dial), agree(dial), badnews(dial), clear(dial), compliment(dial),\n",
    "                                goodnews(dial), guess(dial), hesitate(dial), interrupt(dial), negate(dial), offer(dial),\n",
    "                                pardon(dial), promise(dial), refuse(dial), sympathy(dial), thank(dial)])\n",
    "        words = clean(str(dial).strip().lower())\n",
    "        max_words.append(len(words))\n",
    "        cleaned = nlp(' '.join(c for c in words))\n",
    "        dial_words = []\n",
    "        dial_word_feats = []\n",
    "        for word in cleaned:\n",
    "            dial_words.append(word.text)\n",
    "            word_feat = [word.pos_, word.ent_type_, word.dep_]\n",
    "            dial_word_feats.append(word_feat)\n",
    "            total_word_features.append(word_feat)\n",
    "        train_words.extend(dial_words)\n",
    "        call_words.append(dial_words)\n",
    "        call_word_feats.append(dial_word_feats)\n",
    "    train_dials.extend(call_dials)\n",
    "    corpus.append(call_words)\n",
    "    word_features.append(call_word_feats)\n",
    "    dials.append(call_dials)\n",
    "    dial_features.append(call_dial_feats)\n",
    "    convs.append(call)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialogue_data = [TaggedDocument(words=clean(d), tags=[str(i)]) for i, d in enumerate(train_dials)]\n",
    "conversation_data = [TaggedDocument(words=clean(d), tags=[str(i)]) for i, d in enumerate(convs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_model = Word2Vec([train_words],\n",
    "                       min_count=1,\n",
    "                       size=64,\n",
    "                       workers=2,\n",
    "                       window=5,\n",
    "                       iter=30)\n",
    "\n",
    "dialogue_model = Doc2Vec(min_count=1,\n",
    "                         vector_size=128,\n",
    "                         workers=2,\n",
    "                         window=5,\n",
    "                         epcohs=30)\n",
    "dialogue_model.build_vocab(dialogue_data)\n",
    "dialogue_model.train(dialogue_data,\n",
    "                     total_examples=dialogue_model.corpus_count,\n",
    "                     epochs=dialogue_model.epochs)\n",
    "\n",
    "call_model = Doc2Vec(min_count=1,\n",
    "                     vector_size=128,\n",
    "                     workers=2,\n",
    "                     window=5,\n",
    "                     epochs=30)\n",
    "call_model.build_vocab(conversation_data)\n",
    "call_model.train(conversation_data,\n",
    "                 total_examples=call_model.corpus_count,\n",
    "                 epochs=call_model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_embeddings = []\n",
    "for data in convs:\n",
    "    emb = call_model.infer_vector(clean(data))\n",
    "    conv_embeddings.append(emb)\n",
    "\n",
    "conv_embeddings = pad_sequences(conv_embeddings)\n",
    "    \n",
    "max_dial_length = max(n.shape for n in np.array(train_dials))\n",
    "max_sent_length = 0\n",
    "\n",
    "dial_embeddings = []\n",
    "for data in dials:\n",
    "    dial_emb = []\n",
    "    for dialogue in data:\n",
    "        emb = dialogue_model.infer_vector(clean(dialogue))\n",
    "        dial_emb.append(emb)\n",
    "    dial_embeddings.append(dial_emb)\n",
    "\n",
    "dial_embeddings = pad_sequences(dial_embeddings)\n",
    "\n",
    "word_embeddings = []\n",
    "for data in corpus:\n",
    "    dial_emb = []\n",
    "    for dialogue in data:\n",
    "        word_emb = []\n",
    "        for word in dialogue:\n",
    "            emb = words_model.wv[word]\n",
    "            word_emb.append(emb)\n",
    "        if len(dialogue) == 0:\n",
    "            word_emb.append(np.zeros(shape=(64,)))\n",
    "        dial_emb.append(word_emb)\n",
    "    dial_emb = pad_sequences(dial_emb, maxlen=max(max_words))\n",
    "    word_embeddings.append(dial_emb)\n",
    "\n",
    "word_embeddings = pad_sequences(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_features = pad_sequences(conv_features)\n",
    "\n",
    "binned_dial_features = []\n",
    "for dial in dial_features:\n",
    "    dial_feats = []\n",
    "    for fea in dial:\n",
    "        if fea[0] == 0:\n",
    "            speaker = [1, 0, 0]\n",
    "        elif fea[0] == 1:\n",
    "            speaker = [0, 1, 0]\n",
    "        else:\n",
    "            speaker = [0, 0, 1]\n",
    "        if fea[1] < 2:\n",
    "            hold = [1, 0, 0, 0, 0]\n",
    "        elif fea[1] < 4:\n",
    "            hold = [0, 1, 0, 0, 0]\n",
    "        elif fea[1] < 9:\n",
    "            hold = [0, 0, 1, 0, 0]\n",
    "        elif fea[1] < 241:\n",
    "            hold = [0, 0, 0, 1, 0]\n",
    "        else:\n",
    "            hold = [0, 0, 0, 0, 1]\n",
    "        total = []\n",
    "        total.extend(speaker)\n",
    "        total.extend(hold)\n",
    "        dial_feats.append(total)\n",
    "    binned_dial_features.append(dial_feats)\n",
    "\n",
    "binned_dial_features = pad_sequences(binned_dial_features)\n",
    "\n",
    "enc = OneHotEncoder(handle_unknown='ignore')\n",
    "enc.fit(total_word_features)\n",
    "\n",
    "binned_word_features = []\n",
    "for data in word_features:\n",
    "    dial_feat = []\n",
    "    for dialogue in data:\n",
    "        word_feat = []\n",
    "        for word in dialogue:\n",
    "            feat = enc.transform([word]).toarray().flatten()\n",
    "            word_feat.append(feat)\n",
    "        if len(dialogue) == 0:\n",
    "            word_feat.append(np.zeros(shape=(75, )))\n",
    "        dial_feat.append(word_feat)\n",
    "    dial_feat = pad_sequences(dial_feat, maxlen=max(max_words))\n",
    "    binned_word_features.append(dial_feat)\n",
    "\n",
    "binned_word_features = pad_sequences(binned_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SEQUENTIAL KERAS MODEL!\n",
    "\n",
    "call_emb_layer_input = Input(shape=(128, ))\n",
    "call_emb_layer = Embedding(input_dim=128, output_dim=128)(call_emb_layer_input)\n",
    "call_emb_layer = Flatten()(call_emb_layer)\n",
    "call_emb_layer = Dense(64, activation=\"relu\")(call_emb_layer)\n",
    "call_emb_layer = Dense(32, activation=\"relu\")(call_emb_layer)\n",
    "call_emb_layer = Dense(16, activation=\"relu\")(call_emb_layer)\n",
    "call_emb_layer = Dense(8, activation=\"relu\")(call_emb_layer)\n",
    "\n",
    "call_feat_layer_input = Input(shape=(5, ))\n",
    "call_feat_layer = Dense(16, activation=\"relu\")(call_feat_layer_input)\n",
    "call_feat_layer = Dense(8, activation=\"relu\")(call_feat_layer)\n",
    "\n",
    "merge_all_layers_1 = concatenate([call_emb_layer, call_feat_layer])\n",
    "merge_all_layers_1 = Dense(4, input_dim=8)(merge_all_layers_1)\n",
    "\n",
    "dial_emb_layer_input = Input(shape=(193, 128))\n",
    "dial_emb_layer = Flatten()(dial_emb_layer_input)\n",
    "dial_emb_layer = Embedding(input_dim=193, output_dim=32)(dial_emb_layer)\n",
    "dial_emb_layer = Flatten()(dial_emb_layer)\n",
    "dial_emb_layer = Dense(32, activation=\"relu\")(dial_emb_layer)\n",
    "dial_emb_layer = Dense(16, activation=\"relu\")(dial_emb_layer)\n",
    "dial_emb_layer = Dense(8, activation=\"relu\")(dial_emb_layer)\n",
    "\n",
    "dial_feat_layer_input = Input(shape=(193, 8))\n",
    "dial_feat_layer = Flatten()(dial_feat_layer_input)\n",
    "dial_feat_layer = Dense(8, activation=\"relu\")(dial_feat_layer)\n",
    "\n",
    "merge_all_layers_2 = concatenate([dial_emb_layer, dial_feat_layer, merge_all_layers_1])\n",
    "merge_all_layers_2 = Dense(4, input_dim=8)(merge_all_layers_2)\n",
    "\n",
    "word_emb_layer_input = Input(shape=(193, 222, 64))\n",
    "word_emb_layer = Flatten()(word_emb_layer_input)\n",
    "word_emb_layer = Embedding(input_dim=222, output_dim=32)(word_emb_layer)\n",
    "word_emb_layer = Flatten()(word_emb_layer)\n",
    "word_emb_layer = Dense(32, activation=\"relu\")(word_emb_layer)\n",
    "word_emb_layer = Dense(16, activation=\"relu\")(word_emb_layer)\n",
    "word_emb_layer = Dense(8, activation=\"relu\")(word_emb_layer)\n",
    "\n",
    "word_feat_layer_input = Input(shape=(193, 222, 75))\n",
    "word_feat_layer = Flatten()(word_feat_layer_input)\n",
    "word_feat_layer = Dense(32, activation=\"relu\")(word_feat_layer)\n",
    "word_feat_layer = Dense(16, activation=\"relu\")(word_feat_layer)\n",
    "word_feat_layer = Dense(8, activation=\"relu\")(word_feat_layer)\n",
    "\n",
    "merge_all_layers_3 = concatenate([word_emb_layer, word_feat_layer, merge_all_layers_2])\n",
    "merge_all_layers_3 = Dense(4, input_dim=8)(merge_all_layers_3)\n",
    "merge_all_layers_3 = Dense(2, input_dim=4, activation=\"relu\")(merge_all_layers_3)\n",
    "merge_all_layers_3 = Dense(1, input_dim=2, activation=\"sigmoid\")(merge_all_layers_3)\n",
    "\n",
    "seq_model = Model(inputs=[call_emb_layer_input, call_feat_layer_input, dial_emb_layer_input, dial_feat_layer_input, word_emb_layer_input, word_feat_layer_input],\n",
    "              outputs=merge_all_layers_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seq_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels = []\n",
    "for row in labels_df.values:\n",
    "    y_labels.append(int(row[15]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_conv_embeddings = np.array(conv_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_conv_features = np.array(conv_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_dial_embeddings = np.array(dial_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_binned_dial_features = np.array(dial_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_word_embeddings = np.array(word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_binned_word_features = np.array(binned_word_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "seq_model.fit([conv_embeddings, \n",
    "               conv_features, \n",
    "               dial_embeddings, \n",
    "               binned_dial_features, \n",
    "               word_embeddings, \n",
    "               binned_word_features], \n",
    "              y_labels, \n",
    "              epochs=1, \n",
    "              verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = seq_model.evaluate([conv_embeddings, \n",
    "               conv_features, \n",
    "               dial_embeddings, \n",
    "               binned_dial_features, \n",
    "               word_embeddings, \n",
    "               binned_word_features], \n",
    "              y_labels, \n",
    "              verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRU"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
